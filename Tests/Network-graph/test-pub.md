## Мониторинг и построение графиков заполнения кластера данными c использовании только публичной сети##

### Цель: анализ трафика, проходящего через публичную и кластерную сеть при заполнении CephFS большим файлом###

### Исходные данные ###

**Виртуальные машины:**

	h201: vda(50Gb)-система vdb(100Gb)-OSD
		  eth0:10.0.2.201/24(публичная) eth1:10.0.3.201/24(кластерная)
	h202: vda(50Gb)-система vdb(100Gb)-OSD
		  eth0:10.0.2.202/24(публичная) eth1:10.0.3.202/24(кластерная)
	h203: vda(50Gb)-система vdb(100Gb)-OSD
		  eth0:10.0.2.203/24(публичная) eth1:10.0.3.203/24(кластерная)

**Конфигурационный файл сeph.conf**

***(перед тестом были закоментированы на всех хостах строки public_network,cluster_network и перезапущены сервисы Ceph)***

	
	[global]
	fsid = 5231a9c6-2a1f-4c49-bb1f-4946f35fd408
	# public_network = 10.0.2.0/24
	# cluster_network = 10.0.3.0/24
	mon_initial_members = h201, h202, h203
	mon_host = 10.0.2.201,10.0.2.202,10.0.2.203
	auth_cluster_required = cephx
	auth_service_required = cephx
	auth_client_required = cephx

**Точка монтирования:**

	mount -t ceph h201,h202,h203:/ /mnt/ceph -o name=cephfs,secret=<KEY>	

**Заполнение данными производится c помощью команда dd**

	dd if=/dev/zero of=/mnt/ceph/test.img count=50 bs=1G status=progress    

Начало процесса :   00:40
Окончание процесса: 02:05

**Графики теста:**

	test-pub-graph.pdf

### Итог: ###

На графиках мы видим, что все действия по репликации происходят по публичной сети eth0:10.0.2.0/24, а по сети eth1:10.0.3.0/24 трафик отсутствует.

