## Мониторинг и построение графиков заполнения кластера данными c использовании кластерной и публичной сетей##

### Цель: анализ трафика, проходящего через публичную и кластерную сеть при заполнении CephFS большим файлом### 

### Исходные данные ###

**Виртуальные машины:**

	h201: vda(50Gb)-система vdb(100Gb)-OSD
		  eth0:10.0.2.201/24(публичная) eth1:10.0.3.201/24(кластерная)
	h202: vda(50Gb)-система vdb(100Gb)-OSD
		  eth0:10.0.2.202/24(публичная) eth1:10.0.3.202/24(кластерная)
	h203: vda(50Gb)-система vdb(100Gb)-OSD
		  eth0:10.0.2.203/24(публичная) eth1:10.0.3.203/24(кластерная)

**Конфигурационный файл сeph.conf**
	
	[global]
	fsid = 5231a9c6-2a1f-4c49-bb1f-4946f35fd408
	public_network = 10.0.2.0/24
	cluster_network = 10.0.3.0/24
	mon_initial_members = h201, h202, h203
	mon_host = 10.0.2.201,10.0.2.202,10.0.2.203
	auth_cluster_required = cephx
	auth_service_required = cephx
	auth_client_required = cephx

**Точка монтирования:**

	mount -t ceph h201,h202,h203:/ /mnt/ceph -o name=cephfs,secret=<KEY>	

**Заполнение данными производится c помощью команда dd**

	dd if=/dev/zero of=/mnt/ceph/test.img count=50 bs=1G status=progress    

Начало процесса :   16:00 
Окончание процесса: 17:00

**Графики теста:**

	test-clu-graph.pdf

### Итог: ###

На графиках мы наглядно видим, как при заполнении кластера вся запись реплик на соседние OSD идет по кластерной сети eth1:10.0.3.0/24. При проведении теста в конфигурационном файле сeph.conf не указывались IP каждого OSD и мониторов.

